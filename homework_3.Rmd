---
title: "Hw 3"
author: "Ben Chu"
date: "April 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Loading some data,packages, and function.
```{r warnings = FALSE, messages = FALSE}
library(tidyverse)
library(psych)
library(purrr)
library(stats)
library(DescTools)
library(car)
library(jtools)
library(lmSupport)
library(lm.beta)
library(lsr)
library(QuantPsyc)
load("C:/Users/Branly Mclanbry/Downloads/GSS1991HW.RData")
load("C:/Users/Branly Mclanbry/Downloads/TOP2003.RData")
hw1 <- GSS1991HW %>%
  janitor::clean_names()
options(contrasts = c('contr.helmert', 'contr.poly'))
pphehe <- function(x,var) {
 (qqnorm(x, main = var))
 (qqline(x))
}
denss <- function(x,var) {
  plot(density(x), main = var)
}

skurt.1 <-function(x,var) {
  print(var)
  print(round(DescTools::Kurt(x, method = 2, conf.level = .99, R = 2000),2))
  print(round(DescTools::Skew(x, method = 2, conf.level = .99, R = 2000),2))
}

transformer <- function(x,var){
  squareroot <- (x+1)^.5
  inverse <- 1/(x+1)
  log <- log10(x+1)
  print(var)
  print("squareroot")
  print(round(DescTools::Skew(squareroot,na.rm=TRUE, method=2,conf.level=.99, R=2000),2))
  print(round(DescTools::Kurt(squareroot,na.rm=TRUE, method=2,conf.level=.99, R=2000),2))
  print("log")
  print(round(DescTools::Skew(log,na.rm=TRUE, method=2,conf.level=.99, R = 2000),2))
  print(round(DescTools::Kurt(log,na.rm=TRUE, method=2,conf.level=.99, R = 2000),2))
  print("inverse")
  print(round(DescTools::Skew(inverse,na.rm=TRUE, method=2,conf.level=.99, R = 2000),2))
  print(round(DescTools::Kurt(inverse,na.rm=TRUE, method=2,conf.level=.99, R = 2000),2))
}

p_list <- list(hw1$educ,hw1$maeduc,hw1$prestg80)
p_names <- names(hw1[2:4])
```
#HW1
Sex and education both significantly predict prestige _R_^2^ = .26, _F_(3,1158) = 134.32, _p_ < .001. However, not all variables contributed equally, education was a significant predictor (_b_* = .50, _p_ <.001) while sex was not (_b_* = -.11, _p_ = .73), and neither was the interaction (_b_* = .02, _p_ = .06)
  
Essentially, higher education predicts a higher occupational prestige, while sex and the interaction between sex and eduction do not. 
```{r warnings = FALSE, message = FALSE}
gss.dat <- lm(prestg80~sex*educ, hw1)
summ(gss.dat,center = TRUE, digits = 5, confint = TRUE)
lm.beta(gss.dat)
modelEffectSizes(gss.dat)

ggplot(gss.dat, aes(x = educ, prestg80, color = sex)) +
  geom_smooth(aes(linetype = sex), method = 'lm', se = F)
```


#HW2

Mothers education and occupational prestige were transformed due to a violation in normality. Specifically a reflected square root for mother's education and a square root transformtion for prestige. Mothers education and education significantly predice prestige _R_^2^ = .23, _F_(3,1158) = 115, _p_ < .001. Education level(_b_* = .51, _p_ <.001) and mother's education (_b_* = .07, _p_ < . 01) significantly predicted occupational prestige. It is important to mention that mother's education was reflected so interpretation should be reversed.  
The interaction was also significant (_b_* = .02, _p_ < .001) and the slope for mother's education one standard deviation above the mean (_b_ = 0.22, _p_ < .001), while at the mean (_b_ = .25,_p_ < .001) and one standard deviation below the mean (_b_ = .27, p<.001). Specifically that lower education provides a lower amount of prestige compared to higher education, which provides a larger amount of prestive.
```{r message = FALSE,warning = FALSE}
walk2(p_list,p_names,pphehe)
walk2(p_list,p_names,denss)
walk2(p_list,p_names,skurt.1)
walk2(p_list,p_names,transformer)
```  
##Creating new variables
```{r}
hw1 <- hw1 %>%
  mutate(prestg80_sqrt = sqrt(prestg80),
         maeduc_sqrt_ref = sqrt((max(maeduc) + 1) - maeduc))
```
##Running analysis
```{r}
gss.dat.2 <- lm(prestg80_sqrt~educ*maeduc_sqrt_ref,hw1)
summ(gss.dat.2, center = TRUE, digits = 5, confint = TRUE)
gss.dat.2 %>% center_lm() %>% lm.beta()
sim_slopes(gss.dat.2,maeduc_sqrt_ref,educ,johnson_neyman = FALSE, cont.int = TRUE, centered = c('educ','maeduc_sqrt_ref'), digits = 5)
interact_plot(gss.dat.2,maeduc_sqrt_ref, educ,centered = c('educ','maeduc_sqrt_ref'))

```

#HW3
Normality of residuals suggests that multiple points stray from the line, which might suggest a problem, further analysis are necessary.
Linearity of the residuals from the scale-location, and residuals vs fitted graph are somewhat straight but require further analysis. 
Homoscedasticity is examined through the Breusch-pagan test which is significant, which suggests that homoscedasticity is met. 
Multicollinearity is examined through inflation and tolerance factors which are within margins.
```{r}
plot(gss.dat.2)
n.2 <- 1162
hat <- hatvalues(gss.dat.2)
mahun <- ((n.2-1)*(hat))-1
tail(sort(mahun),10)
1-pchisq(201.45,df = 3)
vif(gss.dat.2)
1/vif(gss.dat.2)
lmtest::bptest(gss.dat.2,varformula = ~fitted.values(gss.dat.2),FALSE,hw1)
```

#HW4
##4A
Results from the ANCOVA suggests that employees make a larger begining salary based off of education _F_ = 302.80(1,471), _p_ <.001.  It is important to look at the effect sizes, specfically that education ($\eta^2$ = .38) accounts for a larger amount of explanation compared to minority status ($\eta^2$ = .01)

Loading in data
```{r}
load("C:/Users/Branly Mclanbry/Downloads/employee (7).RData")
hw2 <- employee %>%
  mutate(educ.num = as.numeric(educ))
```
Ancova model
```{r}
salary.dat2 <- aov(salbegin~educ+minority,hw2)
Anova(salary.dat2, type = "III")
etaSquared(salary.dat2)
```
##4B
Significant interaction suggests that ANCOVA assumptions are violated.
```{r}
salary.dat2 <- aov(salbegin~educ*minority,employee)
Anova(salary.dat2, type = "III")
```
##4C
Still, running the model as a linear model provides slightly different results _R_^2^ = .43, _F_(3.470) = 119.08, _p_ < .001. However, this suggests that education (_b_* = .69, _p_ <.001), minority (_b_* = .71, _p_ < .001) and the interaction (_b_* = -.42, _p_<.001) predicts beginning salary.

Essentially, higher levels of education lead to a higher beginning salary especially if you are a non-minority. Those in the minority status with equivalent education start at a lower salary. 
```{r}
salary.lm<- lm(salbegin ~ educ * minority,employee)
summ(salary.lm, center = TRUE, digits = 5, confint = TRUE)
lm.beta(salary.lm)

sim_slopes(salary.lm,minority,educ,johnson_neyman = FALSE, cont.int = TRUE, centered = c('educ','minority'), digits = 5)

ggplot(employee, aes(educ, salbegin)) +
  geom_smooth(aes(color = minority), method = "lm", se = F)
```

#5 
Authors tested ANCOVA by utilizing regression to test interactions between grade and condition on performance. This was met because the interaction was not significant _p_ = .40 . ANCOVA suggests fairly similar results _F_(1,22) = 3.92, _p_ = .06
```{r}
h5 <- TOP2003 %>% janitor::clean_names()
h5.2 <- aov(quiz2 ~ current + condit, data = h5)
Anova(h5.2)
h5.3 <- aov(quiz2 ~ current * condit, data = h5)
summary(h5.3)
```

#HW6
Power analysis suggests sample size around 140 for significant beta for quality.

```{r include = FALSE}
load("C:/Users/Branly Mclanbry/Downloads/grants.RData")
require(MASS)
pwr.MRC<-function(ry1=NULL, ry2=NULL, ry3=NULL, r12=NULL, r13=NULL, r23=NULL,n=NULL, alpha=.05, rep = 10000, my=0,m1=0,m2=0,m3=0, sy=1,s1=1,s2=1,s3=1){

pred<-NA
pred[is.null(r23)]<-2
pred[!is.null(r23)]<-3

  if (pred=="2")
    {pop <- mvrnorm(n, mu = c(my, m1, m2), Sigma = matrix(c(sy, ry1, ry2,
                                                                     ry1, s1, r12,
                                                                     ry2, r12, s2),
                                                                   ncol = 3), empirical = TRUE)
    pop2 = data.frame(pop)

    values<-lm(X1~X2+X3, pop2)
    values<-summary(values)

    int<-(values$coefficients)[1,3]
    tb1<-(values$coefficients)[2,3] #grabs t from each analysis
    tb2<-(values$coefficients)[3,3]
    R2<-values$r.squared
    F<-values$fstatistic[1]
    df1<-values$fstatistic[2]
    df2<-values$fstatistic[3]

    f2<-R2/(1-R2)
    lambdaR2<-f2*df2
    minusalpha<-1-alpha
    FtR2<-qf(minusalpha, df1, df2)
    powerR2<-round(1-pf(FtR2, df1,df2,lambdaR2),3)

    lambdab1<-tb1^2
    lambdab2<-tb2^2
    Fb<-qf(minusalpha, 1, df2)
    powerb1<-round(1-pf(Fb, 1,df2,lambdab1),3)
    powerb2<-round(1-pf(Fb, 1,df2,lambdab2),3)

    {print(paste("Sample size is ",n))}
    {print(paste("Power R2 = ", powerR2))}
    {print(paste("Power b1 = ", powerb1))}
    {print(paste("Power b2 = ", powerb2))}
     }

  if (pred=="3")
    {
  pop <- mvrnorm(n, mu = c(my, m1, m2, m3), Sigma = matrix(c(sy, ry1, ry2, ry3, ry1, s1, r12, r13, ry2, r12,s2, r23, ry3, r13, r23, s3),
                 ncol = 4), empirical = TRUE)
  pop2 = data.frame(pop)

  values<-lm(X1~X2+X3+X4, pop2)
  values<-summary(values)

  int<-(values$coefficients)[1,3]
  tb1<-(values$coefficients)[2,3] #grabs t from each analysis
  tb2<-(values$coefficients)[3,3]
  tb3<-(values$coefficients)[4,3]
  R2<-values$r.squared
  F<-values$fstatistic[1]
  df1<-values$fstatistic[2]
  df2<-values$fstatistic[3]

  f2<-R2/(1-R2)
  lambdaR2<-f2*df2
  minusalpha<-1-alpha
  FtR2<-qf(minusalpha, df1, df2)
  powerR2<-round(1-pf(FtR2, df1,df2,lambdaR2),3)

  lambdab1<-tb1^2
  lambdab2<-tb2^2
  lambdab3<-tb3^2
  Fb<-qf(minusalpha, 1, df2)
  powerb1<-round(1-pf(Fb, 1,df2,lambdab1),3)
  powerb2<-round(1-pf(Fb, 1,df2,lambdab2),3)
  powerb3<-round(1-pf(Fb, 1,df2,lambdab3),3)


  {print(paste("Sample size is ",n))}
  {print(paste("Power R2 = ", powerR2))}
  {print(paste("Power b1 = ", powerb1))}
  {print(paste("Power b2 = ", powerb2))}
  {print(paste("Power b3 = ", powerb3))}

  }}


pwr.MRC_all<-function(ry1=NULL, ry2=NULL, ry3=NULL, r12=NULL, r13=NULL, r23=NULL,n=NULL, alpha=.05, rep = 10000,my=0,m1=0,m2=0,m3=0, sy=1,s1=1,s2=1,s3=1){

pred<-NA
pred[is.null(r23)]<-2
pred[!is.null(r23)]<-3
  
  if (pred=="2")
    {pop <- mvrnorm(100000, mu = c(my, m1, m2), Sigma = matrix(c(sy, ry1, ry2, 
                                                                     ry1, s1, r12, 
                                                                     ry2, r12, s2),
                                                                   ncol = 3), empirical = TRUE)
    pop2 = data.frame(pop)
    nruns = rep
    int = numeric(nruns)
    b1 = numeric(nruns)
    b2 = numeric(nruns)
    R2 = numeric(nruns)
    F = numeric(nruns)
    df1 = numeric(nruns)
    df2 = numeric(nruns)
    for (i in 1:nruns)
    {samp <- pop2[ sample(nrow(pop2), n), ]
    test <- lm(formula = X1 ~ X2+ X3, data = samp)
    c<-summary(test)
    int[i] = coef(summary(test))[1,4]
    b1[i] = coef(summary(test))[2,4] #grabs p from each analysis
    b2[i] = coef(summary(test))[3,4]
    R2[i] = c$r.squared
    F[i]<-c$fstatistic[1]
    df1[i]<-c$fstatistic[2]
    df2[i]<-c$fstatistic[3]}
    Powerall = data.frame(int = int, b1 = b1, b2 = b2)
    Powerall[4:5, "rejectb1"]<-NA
    Powerall$rejectb1 [ b1 < alpha] <- 1
    Powerall$rejectb1 [ b1 >= alpha] <- 0
    Powerall[4:5, "rejectb2"]<-NA
    Powerall$rejectb2 [ b2 < alpha] <- 1
    Powerall$rejectb2 [ b2 >= alpha] <- 0
    Powerall[4:5, "rejecttotal"]<-NA
    Powerall$rejectall <- (Powerall$rejectb1+ Powerall$rejectb2)

    Reject.None <-NA
    Reject.None [Powerall$rejectall == 0]<-1
    Reject.None [Powerall$rejectall > 0]<-0
    Reject.One <-NA
    Reject.One [Powerall$rejectall == 1]<-1
    Reject.One [Powerall$rejectall != 1]<-0
    Reject.All <-NA
    Reject.All [Powerall$rejectall == 2]<-1
    Reject.All [Powerall$rejectall != 2]<-0
    is.numeric(Reject.None)
    is.numeric(Reject.One)
    is.numeric(Reject.All)
    
    Power_b1<-mean(Powerall$rejectb1)
    Power_b2<-mean(Powerall$rejectb2)
    pR2<-1-pf(F,df1, df2)
    Powerall$rejectR2 [pR2 < alpha] <- 1
    Powerall$rejectR2 [pR2 >= alpha] <- 0
    Power_R2<-mean(Powerall$rejectR2)
    PowerAll_R0<-mean(Reject.None)
    PowerAll_R1<-mean(Reject.One)
    PowerAll_R2<-mean(Reject.All)
    

    {print(paste("Sample size is ",n))}
    {print(paste("Power R2 = ", Power_R2))}
    {print(paste("Power b1 = ", Power_b1))}
    {print(paste("Power b2 = ", Power_b2))}
    {print(paste("Proportion Rejecting None = ", PowerAll_R0))}
    {print(paste("Proportion Rejecting One = ", PowerAll_R1))}
    {print(paste("Power ALL (Proportion Rejecting All) = ", PowerAll_R2))}
     }
  
  if (pred=="3")
    {
  pop <- mvrnorm(100000, mu = c(my, m1, m2, m3), Sigma = matrix(c(sy, ry1, ry2, ry3, ry1, s1, r12, r13, ry2, r12,s2, r23, ry3, r13, r23, s3),
                 ncol = 4), empirical = TRUE)
  pop2 = data.frame(pop)
  nruns = rep
  int = numeric(nruns)
  b1 = numeric(nruns)
  b2 = numeric(nruns)
  b3 = numeric(nruns)
  R2 = numeric(nruns)
  F = numeric(nruns)
  df1 = numeric(nruns)
  df2 = numeric(nruns)
  for (i in 1:nruns)
  {samp <- pop2[ sample(nrow(pop2), n), ]
  test <- lm(formula = X1 ~ X2+ X3+ X4, data = samp)
  c<-summary(test)
  int[i] = coef(summary(test))[1,4]
  b1[i] = coef(summary(test))[2,4] #grabs p from each analysis
  b2[i] = coef(summary(test))[3,4]
  b3[i] = coef(summary(test))[4,4]
  R2[i] = c$r.squared
  F[i]<-c$fstatistic[1]
  df1[i]<-c$fstatistic[2]
  df2[i]<-c$fstatistic[3]}
  Powerall = data.frame(int = int, b1 = b1, b2 = b2, b3 = b3)
  Powerall[4:5, "rejectb1"]<-NA
  Powerall$rejectb1 [ b1 < alpha] <- 1
  Powerall$rejectb1 [ b1 >= alpha] <- 0
  Powerall[4:5, "rejectb2"]<-NA
  Powerall$rejectb2 [ b2 < alpha] <- 1
  Powerall$rejectb2 [ b2 >= alpha] <- 0
  Powerall[4:5, "rejectb3"]<-NA
  Powerall$rejectb3 [ b3 < alpha] <- 1
  Powerall$rejectb3 [ b3 >= alpha] <- 0
  Powerall[4:5, "rejecttotal"]<-NA
  Powerall$rejectall <- (Powerall$rejectb1+ Powerall$rejectb2+ Powerall$rejectb3)

  Reject.None <-NA
  Reject.None [Powerall$rejectall == 0]<-1
  Reject.None [Powerall$rejectall > 0]<-0
  Reject.One <-NA
  Reject.One [Powerall$rejectall == 1]<-1
  Reject.One [Powerall$rejectall != 1]<-0
  Reject.Two <-NA
  Reject.Two [Powerall$rejectall == 2]<-1
  Reject.Two [Powerall$rejectall != 2]<-0
  Reject.All <-NA
  Reject.All [Powerall$rejectall == 3]<-1
  Reject.All [Powerall$rejectall != 3]<-0
  is.numeric(Reject.None)
  is.numeric(Reject.One)
  is.numeric(Reject.Two)
  is.numeric(Reject.All)

  Power_b1<-mean(Powerall$rejectb1)
  Power_b2<-mean(Powerall$rejectb2)
  Power_b3<-mean (Powerall$rejectb3)
  pR2<-1-pf(F,df1, df2)
  Powerall$rejectR2 [pR2 < alpha] <- 1
  Powerall$rejectR2 [pR2 >= alpha] <- 0
  Power_R2<-mean(Powerall$rejectR2)
  PowerAll_R0<-mean(Reject.None)
  PowerAll_R1<-mean(Reject.One)
  PowerAll_R2<-mean(Reject.Two)
  PowerAll_R3<-mean(Reject.All)

 
  {print(paste("Sample size is ",n))}
  {print(paste("Power R2 = ", Power_R2))}
  {print(paste("Power b1 = ", Power_b1))}
  {print(paste("Power b2 = ", Power_b2))}
  {print(paste("Power b3 = ", Power_b3))}
  {print(paste("Proportion Rejecting None = ", PowerAll_R0))}
  {print(paste("Proportion Rejecting One = ", PowerAll_R1))}
  {print(paste("Proportion Rejecting Two = ", PowerAll_R2))}
  {print(paste("Power ALL (Proportion Rejecting All) = ", PowerAll_R3))}
   }}
```
Analysis
```{r}
cor.dat <- cor(grants)
round(cor.dat,2)
pwr.MRC_all(-.24,.45,.56,-.80,-.60,.72,140)
pwr.MRC_all(-.80,-.60,-.24,.72,.45,.56,150)
```
